{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameter(layer):\n",
    "    parameters = {}\n",
    "    Adam_caches = {}\n",
    "    Adam_caches['t'] = tf.Variable(1.0, name='t')\n",
    "    with tf.name_scope('Initial_Parameters'):\n",
    "        for i in range(1, len(layer)):\n",
    "            parameters['W'+str(i)] = tf.Variable(tf.random_normal([layer[i-1], layer[i]], seed=1) * tf.sqrt(2/layer[i-1]), name='W'+str(i))\n",
    "            parameters['b'+str(i)] = tf.Variable(tf.zeros([1, layer[i]]), name='b'+str(i))\n",
    "            Adam_caches['Vdw'+str(i)] = tf.Variable(tf.zeros([layer[i-1], layer[i]]), name='Vdw'+str(i))\n",
    "            Adam_caches['Sdw'+str(i)] = tf.Variable(tf.zeros([layer[i-1], layer[i]]), name='Sdw'+str(i))\n",
    "            Adam_caches['Vdb'+str(i)] = tf.Variable(tf.zeros([1, layer[i]]), name='Vdb'+str(i))\n",
    "            Adam_caches['Sdb'+str(i)] = tf.Variable(tf.zeros([1, layer[i]]), name='Sdb'+str(i))\n",
    "    return parameters, Adam_caches\n",
    "\n",
    "def forward_propagate(train_X, parameters, en_dropout=0, keep_prob=0.5):\n",
    "    network_deep = int(len(parameters)/2)\n",
    "    caches = {}\n",
    "    dropout_layers = {}\n",
    "    caches['A0'] = train_X\n",
    "    for i in range(1, network_deep):\n",
    "        with tf.name_scope('hidden_layer'+str(i)):\n",
    "            caches['Z'+str(i)] = tf.add(tf.matmul(caches['A'+str(i-1)], parameters['W'+str(i)]), parameters['b'+str(i)], name='Z'+str(i))\n",
    "            caches['A'+str(i)] = tf.nn.relu(caches['Z'+str(i)], name='A'+str(i))     \n",
    "            if en_dropout:\n",
    "                caches['A'+str(i)] = tf.nn.dropout(caches['A'+str(i)], keep_prob=keep_prob, name='dropout'+str(i))\n",
    "            # Visualize\n",
    "            tf.summary.histogram('A'+str(i), caches['A'+str(i)])\n",
    "        \n",
    "    with tf.name_scope('output_layer'):\n",
    "        caches['Z'+str(network_deep)] = tf.add(tf.matmul(caches['A'+str(network_deep-1)], parameters['W'+str(network_deep)]), parameters['b'+str(network_deep)], name='Z'+str(network_deep))\n",
    "        caches['Y_hat'] = tf.nn.softmax(caches['Z'+str(network_deep)], name='Y_hat')\n",
    "        # Visualize\n",
    "        tf.summary.histogram('Y_hat'+str(i), caches['Y_hat'])\n",
    "    return caches\n",
    "    \n",
    "def compute_cost(train_Y, Y_hat, parameters=0, reg_rate=0):\n",
    "    with tf.name_scope('compute_cost'):\n",
    "        with tf.name_scope('cross-entropy_cost'):\n",
    "            cost = tf.reduce_mean(-train_Y*tf.log(Y_hat+1e-10) - (1-train_Y)*tf.log(1-Y_hat+1e-10))\n",
    "        if reg_rate != 0:\n",
    "            network_deep = int(len(parameters)/2)\n",
    "            with tf.name_scope('L2_regularization_cost'):\n",
    "                sum_W = 0\n",
    "                for i in range(1, network_deep+1):\n",
    "                    sum_W += tf.nn.l2_loss([parameters['W'+str(i)]])\n",
    "                    sum_W += tf.nn.l2_loss([parameters['b'+str(i)]])\n",
    "                m = tf.cast(tf.shape(parameters['W1'])[0], dtype=tf.float32, name='m')\n",
    "                L2 = sum_W * reg_rate / m\n",
    "            cost = cost + L2\n",
    "        # Visualize\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    return cost\n",
    "\n",
    "def compute_accuracy(train_Y, Y_hat):\n",
    "    with tf.name_scope('compute_accuracy'):\n",
    "        correct = tf.equal(tf.argmax(Y_hat, 1), tf.argmax(train_Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        # Visualize\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def compute_gradients(cost, parameters):\n",
    "    network_deep = int(len(parameters)/2)\n",
    "    grads = {}\n",
    "    for i in range(1, network_deep+1):\n",
    "        grads['grad_W'+str(i)] = tf.gradients(cost, parameters['W'+str(i)], name='grad_W'+str(i))[0]\n",
    "        grads['grad_b'+str(i)] = tf.gradients(cost, parameters['b'+str(i)], name='grad_b'+str(i))[0]\n",
    "        # Visualize\n",
    "        tf.summary.histogram('gradients_W'+str(i), grads['grad_W'+str(i)])\n",
    "        tf.summary.histogram('gradients_b'+str(i), grads['grad_b'+str(i)])\n",
    "    return grads\n",
    "\n",
    "def SGC_optimization(grads, parameters, learning_rate=0.01):\n",
    "    network_deep = int(len(parameters)/2)\n",
    "    new_parameters = {}\n",
    "    for i in range(1, network_deep+1):\n",
    "        with tf.name_scope('Compute_new_W'+str(i)):\n",
    "            new_parameters['new_W'+str(i)] = parameters['W'+str(i)] - learning_rate * grads['grad_W'+str(i)]\n",
    "        with tf.name_scope('Compute_new_b'+str(i)):\n",
    "            new_parameters['new_b'+str(i)] = parameters['b'+str(i)] - learning_rate * grads['grad_b'+str(i)]\n",
    "            \n",
    "        parameters['W'+str(i)] = tf.assign(parameters['W'+str(i)], new_parameters['new_W'+str(i)], name='update_W'+str(i))\n",
    "        parameters['b'+str(i)] = tf.assign(parameters['b'+str(i)], new_parameters['new_b'+str(i)], name='update_b'+str(i))\n",
    "        # Visualize\n",
    "        tf.summary.histogram('W'+str(i), parameters['W'+str(i)])\n",
    "        tf.summary.histogram('b'+str(i), parameters['b'+str(i)])\n",
    "    return parameters\n",
    "\n",
    "def Adam_optimization(grads, parameters, Adam_caches, learning_rate=0.01, beta1=0.9, beta2=0.999):\n",
    "    network_deep = int(len(parameters)/2)\n",
    "    new_parameters = {}\n",
    "    for i in range(1, network_deep+1):\n",
    "        with tf.name_scope('Compute_new_W'+str(i)):\n",
    "            Adam_caches['Vdw'+str(i)] = tf.assign(Adam_caches['Vdw'+str(i)], beta1*Adam_caches['Vdw'+str(i)] + (1.0-beta1)*grads['grad_W'+str(i)])\n",
    "            Adam_caches['Sdw'+str(i)] = tf.assign(Adam_caches['Sdw'+str(i)], beta2*Adam_caches['Sdw'+str(i)] + (1.0-beta2)*grads['grad_W'+str(i)]*grads['grad_W'+str(i)])\n",
    "            Adam_caches['Vdwcorrected'+str(i)] = Adam_caches['Vdw'+str(i)]/(1.0-beta1**Adam_caches['t'])\n",
    "            Adam_caches['Sdwcorrected'+str(i)] = Adam_caches['Sdw'+str(i)]/(1.0-beta2**Adam_caches['t'])\n",
    "            new_parameters['new_W'+str(i)] = parameters['W'+str(i)] - learning_rate * Adam_caches['Vdwcorrected'+str(i)]/tf.sqrt(Adam_caches['Sdwcorrected'+str(i)]+1e-8)\n",
    "        with tf.name_scope('Compute_new_b'+str(i)):\n",
    "            Adam_caches['Vdb'+str(i)] = tf.assign(Adam_caches['Vdb'+str(i)], beta1*Adam_caches['Vdb'+str(i)] + (1.0-beta1)*grads['grad_b'+str(i)])\n",
    "            Adam_caches['Sdb'+str(i)] = tf.assign(Adam_caches['Sdb'+str(i)], beta2*Adam_caches['Sdb'+str(i)] + (1.0-beta2)*grads['grad_b'+str(i)]*grads['grad_b'+str(i)])\n",
    "            Adam_caches['Vdbcorrected'+str(i)] = Adam_caches['Vdb'+str(i)]/(1.0-beta1**Adam_caches['t'])\n",
    "            Adam_caches['Sdbcorrected'+str(i)] = Adam_caches['Sdb'+str(i)]/(1.0-beta2**Adam_caches['t'])\n",
    "            new_parameters['new_b'+str(i)] = parameters['b'+str(i)] - learning_rate * Adam_caches['Vdbcorrected'+str(i)]/tf.sqrt(Adam_caches['Sdbcorrected'+str(i)]+1e-8)\n",
    "        \n",
    "        parameters['W'+str(i)] = tf.assign(parameters['W'+str(i)], new_parameters['new_W'+str(i)], name='update_W'+str(i))\n",
    "        parameters['b'+str(i)] = tf.assign(parameters['b'+str(i)], new_parameters['new_b'+str(i)], name='update_b'+str(i))\n",
    "        # Visualize\n",
    "        tf.summary.histogram('W'+str(i), parameters['W'+str(i)])\n",
    "        tf.summary.histogram('b'+str(i), parameters['b'+str(i)])\n",
    "        tf.summary.histogram('Vdw'+str(i), Adam_caches['Vdw'+str(i)])\n",
    "        tf.summary.histogram('Sdw'+str(i), Adam_caches['Sdw'+str(i)])\n",
    "        tf.summary.histogram('Vdb'+str(i), Adam_caches['Vdb'+str(i)])\n",
    "        tf.summary.histogram('Sdb'+str(i), Adam_caches['Sdb'+str(i)])\n",
    "        \n",
    "    Adam_caches['t'] = tf.assign(Adam_caches['t'], Adam_caches['t'] + 1.0)\n",
    "    # Visualize\n",
    "    tf.summary.scalar('t', Adam_caches['t'])\n",
    "    return parameters\n",
    "\n",
    "def training(cost, parameters, Adam_caches=0, learning_rate=0.01):\n",
    "    with tf.name_scope('training'):\n",
    "        grads = compute_gradients(cost, parameters)\n",
    "        trained = Adam_optimization(grads, parameters, Adam_caches=Adam_caches, learning_rate=learning_rate)\n",
    "        #trained = SGC_optimization(grads, parameters, learning_rate)\n",
    "    return trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_set, layer_info, learning_rate_init=0.01, larning_rate_decay_rate=1, keep_prob=1, reg_rate=0, iteration_st=0, iteration=1000, batch_size = 128, loaddir=None, logname=None):\n",
    "    # Reset Summary Graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Define input\n",
    "    X = tf.placeholder('float', name='X')\n",
    "    Y = tf.placeholder('float', name='Y')\n",
    "    learning_rate = tf.placeholder('float', name='learning_rate')\n",
    "    \n",
    "    # Forward propagation\n",
    "    parameters, Adam_caches = init_parameter(layer_info)\n",
    "    caches = forward_propagate(X, parameters, en_dropout=keep_prob!=1, keep_prob=keep_prob)\n",
    "    cost = compute_cost(Y, caches['Y_hat'], parameters, reg_rate=reg_rate)\n",
    "    accuracy = compute_accuracy(Y, caches['Y_hat'])\n",
    "        \n",
    "    # Training\n",
    "    trained = training(cost, parameters, Adam_caches, learning_rate)\n",
    "    #trained = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    # Visualize\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Initial Save\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    \n",
    "    # Implement mini-batch\n",
    "    train_X_batch, train_Y_batch = tf.train.batch([data_set['train_X'], data_set['train_Y']], batch_size=batch_size, allow_smaller_final_batch=True, enqueue_many=True)\n",
    "\n",
    "    # Run tensorflow\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    config.log_device_placement = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init)\n",
    "        learning_rate_final = learning_rate_init\n",
    "        \n",
    "        #Set date time for Summary\n",
    "        now = datetime.now()\n",
    "        if logname is None:\n",
    "            logdir = './logs/' + str(now)\n",
    "        else:\n",
    "            logdir = './logs/' + logname\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "        \n",
    "        if loaddir is not None:\n",
    "            loaddir = './checkpoint/' + loaddir\n",
    "            saver.restore(sess, loaddir)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord) \n",
    "        \n",
    "        for i in range(iteration_st, iteration+iteration_st+1):\n",
    "            # Learning_rate_decay\n",
    "            if (i%10 == 0) and (i!=0) :\n",
    "                learning_rate_final = learning_rate_final * larning_rate_decay_rate\n",
    "            \n",
    "            batch_num = data_set['train_X'].shape[0]/batch_size\n",
    "            if batch_num % 1 != 0:\n",
    "                batch_num = int(batch_num) + 1\n",
    "            else:\n",
    "                batch_num = int(batch_num)\n",
    "            \n",
    "            for j in range(1, batch_num+1):\n",
    "                # Get mini-batch\n",
    "                train_X_mini, train_Y_mini = sess.run([train_X_batch, train_Y_batch])\n",
    "                \n",
    "                # Run train\n",
    "                sess.run(trained, feed_dict={X:train_X_mini, Y:train_Y_mini, learning_rate:learning_rate_final})\n",
    "                \n",
    "            train_acc = sess.run(accuracy, feed_dict={X:train_X_mini, Y:train_Y_mini})\n",
    "            train_cost = sess.run(cost, feed_dict={X:train_X_mini, Y:train_Y_mini})\n",
    "            dev_acc = sess.run(accuracy, feed_dict={X:data_set['dev_X'], Y:data_set['dev_Y']})\n",
    "            dev_cost = sess.run(cost, feed_dict={X:data_set['dev_X'], Y:data_set['dev_Y']})\n",
    "            \n",
    "            summary = sess.run(merged, feed_dict={X:train_X_mini, Y:train_Y_mini, learning_rate:learning_rate_final})\n",
    "            writer.add_summary(summary, global_step=i)\n",
    "            print(\"epoch: {}, train_cost: {}, train_acc: {:.2f}%, dev_cost: {}, dev_acc: {:.2f}%\".format(i, train_cost, train_acc*100, dev_cost, dev_acc*100))\n",
    "                \n",
    "            # Save checkpoint\n",
    "            savedir = './checkpoint/' + logname\n",
    "            save_path = saver.save(sess, savedir)\n",
    "        \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(loaddir, inputs, labels, layer_info):\n",
    "    #Reset Summary Graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #Define input\n",
    "    X = tf.placeholder('float', name='X')\n",
    "    Y = tf.placeholder('float', name='Y')\n",
    "    \n",
    "    #Initial parameters\n",
    "    parameters, _ = init_parameter(layer_info)\n",
    "    \n",
    "    #Forward Propagation\n",
    "    caches = forward_propagate(X, parameters)\n",
    "        \n",
    "    #Compute Accuracy\n",
    "    accuracy = compute_accuracy(Y, caches['Y_hat'])\n",
    "    \n",
    "    #Initial Save\n",
    "    saver = tf.train.Saver(max_to_keep=0)\n",
    "\n",
    "    #Run tensorflow\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        loaddir = './checkpoint/' + loaddir\n",
    "        saver.restore(sess, loaddir)\n",
    "        \n",
    "        writer = tf.summary.FileWriter('./logs/predict', sess.graph)\n",
    "        \n",
    "        caches_sh, accuracy_sh = sess.run([caches, accuracy], feed_dict={X:inputs, Y:labels})\n",
    "    return caches_sh['Y_hat'], accuracy_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_DIR = '../inputs/emnist-digits-train.csv'\n",
    "DATA_TEST_DIR = '../inputs/emnist-digits-test.csv'\n",
    "\n",
    "# Read CSV\n",
    "csv_read = pd.read_csv(DATA_TRAIN_DIR, header=None, sep=',', dtype='uint8')\n",
    "\n",
    "# Split training set into train and dev sets\n",
    "_labels = csv_read.iloc[:, 0]\n",
    "train_set, dev_set = train_test_split(csv_read, test_size=0.2, random_state=42, stratify=_labels)\n",
    "\n",
    "# Normalization and One-hot encoder\n",
    "train_X = np.array(train_set.iloc[:, 1:] / 255, dtype=np.float32)\n",
    "train_Y = np.array(pd.get_dummies(train_set.iloc[:, 0]), dtype=np.float32)\n",
    "dev_X = np.array(dev_set.iloc[:, 1:] / 255, dtype=np.float32)\n",
    "dev_Y = np.array(pd.get_dummies(dev_set.iloc[:, 0]), dtype=np.float32)\n",
    "\n",
    "# Read CSV\n",
    "csv_read = pd.read_csv(DATA_TEST_DIR, header=None, sep=',', dtype='uint8')\n",
    "\n",
    "# Normalization and One-hot encoder\n",
    "test_X = np.array(train_set.iloc[:, 1:] / 255, dtype=np.float32)\n",
    "test_Y = np.array(pd.get_dummies(train_set.iloc[:, 0]), dtype=np.float32)\n",
    "\n",
    "# Assemble\n",
    "data_set = {'train_X': train_X, 'train_Y': train_Y, 'dev_X': dev_X, 'dev_Y': dev_Y, 'test_X': test_X, 'test_Y': test_Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/teakatz/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/teakatz/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-df6657195351>:28: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From /home/teakatz/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/teakatz/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/teakatz/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/input.py:784: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-df6657195351>:54: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "epoch: 0, train_cost: 0.04639327526092529, train_acc: 97.66%, dev_cost: 0.050553567707538605, dev_acc: 97.03%\n",
      "epoch: 1, train_cost: 0.03702140599489212, train_acc: 98.44%, dev_cost: 0.04233405366539955, dev_acc: 97.66%\n",
      "epoch: 2, train_cost: 0.03259336203336716, train_acc: 99.22%, dev_cost: 0.03814941644668579, dev_acc: 97.96%\n",
      "epoch: 3, train_cost: 0.029835062101483345, train_acc: 99.22%, dev_cost: 0.03516526147723198, dev_acc: 98.14%\n",
      "epoch: 4, train_cost: 0.02755982056260109, train_acc: 99.22%, dev_cost: 0.032896384596824646, dev_acc: 98.22%\n",
      "epoch: 5, train_cost: 0.025604933500289917, train_acc: 99.22%, dev_cost: 0.031068425625562668, dev_acc: 98.31%\n",
      "epoch: 6, train_cost: 0.024052508175373077, train_acc: 99.22%, dev_cost: 0.029606373980641365, dev_acc: 98.37%\n",
      "epoch: 7, train_cost: 0.023115739226341248, train_acc: 99.22%, dev_cost: 0.028455913066864014, dev_acc: 98.40%\n",
      "epoch: 8, train_cost: 0.021828411146998405, train_acc: 99.22%, dev_cost: 0.02739226445555687, dev_acc: 98.41%\n",
      "epoch: 9, train_cost: 0.020978953689336777, train_acc: 99.22%, dev_cost: 0.02651512250304222, dev_acc: 98.44%\n",
      "epoch: 10, train_cost: 0.019984956830739975, train_acc: 99.22%, dev_cost: 0.02584851160645485, dev_acc: 98.44%\n",
      "epoch: 11, train_cost: 0.019145946949720383, train_acc: 100.00%, dev_cost: 0.02519882470369339, dev_acc: 98.44%\n",
      "epoch: 12, train_cost: 0.018428444862365723, train_acc: 100.00%, dev_cost: 0.024669351056218147, dev_acc: 98.45%\n",
      "epoch: 13, train_cost: 0.017904674634337425, train_acc: 100.00%, dev_cost: 0.024209143593907356, dev_acc: 98.46%\n",
      "epoch: 14, train_cost: 0.017217151820659637, train_acc: 100.00%, dev_cost: 0.02380269765853882, dev_acc: 98.47%\n",
      "epoch: 15, train_cost: 0.01699696108698845, train_acc: 100.00%, dev_cost: 0.023464467376470566, dev_acc: 98.47%\n",
      "epoch: 16, train_cost: 0.016589008271694183, train_acc: 100.00%, dev_cost: 0.02311641350388527, dev_acc: 98.49%\n",
      "epoch: 17, train_cost: 0.016214951872825623, train_acc: 100.00%, dev_cost: 0.022807717323303223, dev_acc: 98.50%\n",
      "epoch: 18, train_cost: 0.01583712175488472, train_acc: 100.00%, dev_cost: 0.022604726254940033, dev_acc: 98.48%\n",
      "epoch: 19, train_cost: 0.015670735388994217, train_acc: 100.00%, dev_cost: 0.022368330508470535, dev_acc: 98.49%\n",
      "epoch: 20, train_cost: 0.015342465601861477, train_acc: 100.00%, dev_cost: 0.022172123193740845, dev_acc: 98.51%\n"
     ]
    }
   ],
   "source": [
    "layer_info = [data_set['train_X'].shape[1], 100, 100, 100, 100, data_set['train_Y'].shape[1]]\n",
    "train_model(data_set, layer_info, learning_rate_init=0.001, reg_rate=0.12, iteration_st=0, iteration=20, batch_size=128, logname='EMNISTletters_100_100_100_100_0.12_minibatch128_Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/teakatz/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/EMNISTletters_100_100_100_100_0.12_minibatch128_Adam\n",
      "The train_set accuracy is :  99.12968873977661 %\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/EMNISTletters_100_100_100_100_0.12_minibatch128_Adam\n",
      "The dev_set accuracy is :  98.53125214576721 %\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/EMNISTletters_100_100_100_100_0.12_minibatch128_Adam\n",
      "The test_set accuracy is :  99.12968873977661 %\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "model = 'EMNISTletters_100_100_100_100_0.12_minibatch128_Adam'\n",
    "_, accuracy = predict(model, data_set['train_X'], data_set['train_Y'], layer_info)\n",
    "print('The train_set accuracy is : ', accuracy*100, '%')\n",
    "_, accuracy = predict(model, data_set['dev_X'], data_set['dev_Y'], layer_info)\n",
    "print('The dev_set accuracy is : ', accuracy*100, '%')\n",
    "_, accuracy = predict(model, data_set['test_X'], data_set['test_Y'], layer_info)\n",
    "print('The test_set accuracy is : ', accuracy*100, '%')\n",
    "print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_DIR = '../inputs/emnist-letters-train.csv'\n",
    "DATA_TEST_DIR = '../inputs/emnist-letters-test.csv'\n",
    "\n",
    "# Read CSV\n",
    "csv_read = pd.read_csv(DATA_TRAIN_DIR, header=None, sep=',', dtype='uint8')\n",
    "\n",
    "# Split training set into train and dev sets\n",
    "_labels = csv_read.iloc[:, 0]\n",
    "train_set, dev_set = train_test_split(csv_read, test_size=0.2, random_state=42, stratify=_labels)\n",
    "\n",
    "# Normalization and One-hot encoder\n",
    "train_X = np.array(train_set.iloc[:, 1:] / 255, dtype=np.float32)\n",
    "train_Y = np.array(pd.get_dummies(train_set.iloc[:, 0]), dtype=np.float32)\n",
    "dev_X = np.array(dev_set.iloc[:, 1:] / 255, dtype=np.float32)\n",
    "dev_Y = np.array(pd.get_dummies(dev_set.iloc[:, 0]), dtype=np.float32)\n",
    "\n",
    "# Read CSV\n",
    "csv_read = pd.read_csv(DATA_TEST_DIR, header=None, sep=',', dtype='uint8')\n",
    "\n",
    "# Normalization and One-hot encoder\n",
    "test_X = np.array(train_set.iloc[:, 1:] / 255, dtype=np.float32)\n",
    "test_Y = np.array(pd.get_dummies(train_set.iloc[:, 0]), dtype=np.float32)\n",
    "\n",
    "# Assemble\n",
    "data_set = {'train_X': train_X, 'train_Y': train_Y, 'dev_X': dev_X, 'dev_Y': dev_Y, 'test_X': test_X, 'test_Y': test_Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_cost: 0.08419038355350494, train_acc: 76.56%, dev_cost: 0.07854300737380981, dev_acc: 80.41%\n",
      "epoch: 1, train_cost: 0.07424628734588623, train_acc: 79.69%, dev_cost: 0.07067142426967621, dev_acc: 83.55%\n",
      "epoch: 2, train_cost: 0.06911340355873108, train_acc: 82.81%, dev_cost: 0.06697901338338852, dev_acc: 84.71%\n",
      "epoch: 3, train_cost: 0.06563639640808105, train_acc: 85.94%, dev_cost: 0.06448525935411453, dev_acc: 85.39%\n",
      "epoch: 4, train_cost: 0.06335578858852386, train_acc: 85.94%, dev_cost: 0.06264330446720123, dev_acc: 85.91%\n",
      "epoch: 5, train_cost: 0.06160394102334976, train_acc: 85.94%, dev_cost: 0.0611581951379776, dev_acc: 86.35%\n",
      "epoch: 6, train_cost: 0.060014933347702026, train_acc: 86.72%, dev_cost: 0.05990079045295715, dev_acc: 86.63%\n",
      "epoch: 7, train_cost: 0.05856204777956009, train_acc: 88.28%, dev_cost: 0.05880029872059822, dev_acc: 86.87%\n",
      "epoch: 8, train_cost: 0.0573691762983799, train_acc: 88.28%, dev_cost: 0.05785871669650078, dev_acc: 87.05%\n",
      "epoch: 9, train_cost: 0.0561949796974659, train_acc: 88.28%, dev_cost: 0.05699659883975983, dev_acc: 87.20%\n",
      "epoch: 10, train_cost: 0.0551651269197464, train_acc: 89.06%, dev_cost: 0.05620080605149269, dev_acc: 87.35%\n",
      "epoch: 11, train_cost: 0.054191477596759796, train_acc: 89.06%, dev_cost: 0.05549430474638939, dev_acc: 87.47%\n",
      "epoch: 12, train_cost: 0.05342426151037216, train_acc: 89.06%, dev_cost: 0.05484678968787193, dev_acc: 87.49%\n",
      "epoch: 13, train_cost: 0.05259107053279877, train_acc: 89.84%, dev_cost: 0.05424489825963974, dev_acc: 87.67%\n",
      "epoch: 14, train_cost: 0.05193985998630524, train_acc: 89.84%, dev_cost: 0.05369025468826294, dev_acc: 87.79%\n",
      "epoch: 15, train_cost: 0.05133476108312607, train_acc: 89.84%, dev_cost: 0.05314435064792633, dev_acc: 87.87%\n",
      "epoch: 16, train_cost: 0.05069991946220398, train_acc: 89.84%, dev_cost: 0.052663471549749374, dev_acc: 87.98%\n",
      "epoch: 17, train_cost: 0.050253212451934814, train_acc: 89.84%, dev_cost: 0.05220220983028412, dev_acc: 88.10%\n",
      "epoch: 18, train_cost: 0.049778446555137634, train_acc: 89.84%, dev_cost: 0.05176302045583725, dev_acc: 88.20%\n",
      "epoch: 19, train_cost: 0.04934411495923996, train_acc: 89.84%, dev_cost: 0.05134331062436104, dev_acc: 88.23%\n",
      "epoch: 20, train_cost: 0.04883868247270584, train_acc: 89.84%, dev_cost: 0.050936102867126465, dev_acc: 88.33%\n"
     ]
    }
   ],
   "source": [
    "layer_info = [data_set['train_X'].shape[1], 100, 100, 100, 100, data_set['train_Y'].shape[1]]\n",
    "train_model(data_set, layer_info, learning_rate_init=0.001, reg_rate=0.12, iteration_st=0, iteration=20, batch_size=128, logname='EMNISTletters_100_100_100_100_0.12_minibatch128_Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/EMNISTletters_100_100_100_100_0.12_minibatch128_Adam\n",
      "The train_set accuracy is :  90.63766598701477 %\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/EMNISTletters_100_100_100_100_0.12_minibatch128_Adam\n",
      "The dev_set accuracy is :  88.30518126487732 %\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/EMNISTletters_100_100_100_100_0.12_minibatch128_Adam\n",
      "The test_set accuracy is :  90.63766598701477 %\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "model = 'EMNISTletters_100_100_100_100_0.12_minibatch128_Adam'\n",
    "_, accuracy = predict(model, data_set['train_X'], data_set['train_Y'], layer_info)\n",
    "print('The train_set accuracy is : ', accuracy*100, '%')\n",
    "_, accuracy = predict(model, data_set['dev_X'], data_set['dev_Y'], layer_info)\n",
    "print('The dev_set accuracy is : ', accuracy*100, '%')\n",
    "_, accuracy = predict(model, data_set['test_X'], data_set['test_Y'], layer_info)\n",
    "print('The test_set accuracy is : ', accuracy*100, '%')\n",
    "print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
